\section{Introduction}
\thispagestyle{empty}
Permanent magnet (PM) machines are commonly used in high-performance applications due to numerous favorable characteristics, including high efficiency, power density and reliability. However, PM machines also incorporate inherent torque ripple, which causes mechanical vibrations, acoustic noise and uneven torque production leading to degraded controllability of the machine \cite{ILC:2018}. Inevitably, the performance of PM machines can be improved by minimizing the torque ripple.

The torque ripple originates from various phenomena. In sight of ripple minimization, the most important ripple sources consist of cogging torque, flux harmonics and current measurement errors. Not only have been these sources commonly acknowledged \cite{ILC:2005,ILC:2012,CTR_SW:2017,ILC:2018}, but also their contribution to the ripple can be reduced by improving control of the machine. In addition to these three ripple sources, also others exist, yet these are not considered in this thesis due to significantly lower performance impact.

Many solutions have been proposed for minimizing the torque ripple. In general, these solutions can be divided into two categories \cite{CTR_SW:1993, CTR_SW:1998, CTR_SW:2017, ILC:2012, ILC:2018}. The methods in the first category aim to improve the geometry of the machine, thus leading to reduced torque ripples. Although, these methods are ordinarily effective, the approach can be impractical and costly to realize, if the machine has been manufactured already. The methods in the second category aim to compensate for the ripple by improving control algorithms. These approaches have the benefit of being cost-effective, since software changes can be applied to already existing control platform \cite{ILC:2018}. The thesis focus will be in the methods of the second category, as the objective is to reduce torque ripple occurring in already existing machines.

Iterative learning control (ILC) seems one of the most prominent compensation methods in the light of current research. ILC is easy to take into use, compensation is automatic and the method only requires measuring of the rotor speed. Due to these attractive features, ILC is studied extensively. Unfortunately, ILC also has some shortcomings. For example, the compensator must be tuned carefully and its behaviour is not always fully predictable. Careful gain tuning is time consuming and may require domain expertise. Furthermore, fixed gain values may not work in all operating conditions and hence the gains may require retuning if operating conditions change. Some ILC based compensation schemes have been also patented \cite{ILC:patent, ILC:patent2}, thus potentially restricting use of ILC. 

During recent years, reinforcement learning (RL) has increasingly gained popularity and RL methods have been successfully used for solving various complex problems automatically. In \cite{RL:control, RL:control2}, RL was used to solve control problems, such as balancing an inverted pendulum and controlling a gripper in a simulation environment. In \cite{RL:atari}, RL was used to train an agent to play Atari 2600 games. In many games, the agent was able to achieve a performance level comparable to a professional game tester. In \cite{RL:alphago}, RL was utilized to train an agent capable of defeating the European Go champion. By considering the complexity of the previous problems, it can be reasoned that RL methods could be potentially used for building a compensator.

Q-learning is a reinforcement learning algorithm, which can be used to learn a policy that compensates for torque pulsations. The compensator based on this algorithm can overcome the weaknesses of ILC while also maintaining its key strengths. Similarly to ILC, the algorithm is computationally light, compensation is automatic and the compensator structure is modular. All control actions are known in advance with Q-learning algorithm, which makes it easy to predict the system behaviour. The adjustable parameters are easy to tune and tuning must be performed only once. Despite the numerous benefits, no work has yet attempted to evaluate the applicability of Q-learning in reducing torque ripple in industrial PM motors.

This thesis aims to demonstrate applicability of Q-learning for building a modular compensator that automatically reduces torque ripple. The compensator is implemented and torque ripple reduction is tested with industrial PM motors. The performance of the Q-learning based compensator is evaluated and compared against ILC and convention PI speed control. Therefore, the thesis will also validate the applicability of the ILC method with industrial PM machines.

Various methods are used in order to produce a comprehensive study. Finite element analysis (FEA) is used to validate that cogging torque, flux harmonics and current measurement errors truly give rise to torque ripple. These simulations are time-consuming, hence a faster lumped-element motor control simulator is used for implementing and evaluating the theoretical performance of the compensators. The performance will be evaluated by comparing torque and speed pulsations in case of using conventional PI speed control, PI with ILC and PI with Q-learning based compensator. Finally, experimental tests are performed with 6-kW and 0.72-kW PM motors. The rotor speed is measured with a resolver to be able to evaluate compensation performance.

This thesis is organized as follows. Chapter 2 reviews the important ripple components and the mathematical model underlying PM synchronous motor (PMSM). Chapter 3 outlines the existing torque ripple minimization methods and their limitations. Chapter 4 describes ILC and its utilization for torque ripple minimization. Chapter 5 discusses the Q-learning based compensation scheme. Chapter 6 presents simulation results for ILC and Q-learning based compensation. Chapter 7 presents the experimental results comparing the compensation of conventional PI-control, ILC and Q-learning methods. Chapter 8 concludes by discussing the results and suggesting future work.

\clearpage
